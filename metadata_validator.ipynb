{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata Validator\n",
    "This jupyter notebook is to faciliate development of a gen3 metadata validation script\n",
    "\n",
    "***\n",
    "\n",
    "### General Idea\n",
    "1. Load metadata into a python object \n",
    "    - class for loading and storing metadata\n",
    "    - define input folder, reads in .json, _*.json and dataImportOrder.txt into an accessible object\n",
    "1. Load schema into a python object \n",
    "    - class Loads bundled json and also splits yamls from bundled json into accessible splits\n",
    "1. \n",
    "\n",
    "***\n",
    "\n",
    "# Perplexity help\n",
    "- [link to chat](https://www.perplexity.ai/search/lets-say-I-erdZUVAOQ_SgDnHh_3meWA)\n",
    "\n",
    "\n",
    "To handle a scenario where your `bundled.json` file contains a `_definitions.yaml` file, and some of the YAML schemas in the `bundled.json` link to `_definitions.yaml` for common definitions, you need to ensure that your JSON Schema validator can resolve these references correctly. Here’s how you can achieve this using Python:\n",
    "\n",
    "### Steps to Validate `metadata.json` Against a Schema with External Definitions\n",
    "\n",
    "1. **Load and Parse YAML Files**: Load and parse the `_definitions.yaml` and other YAML schemas from the `bundled.json` file.\n",
    "2. **Resolve References**: Ensure that references to definitions in `_definitions.yaml` are correctly resolved.\n",
    "3. **Validate the JSON Data**: Use a JSON Schema validator to validate the `metadata.json` file against the resolved schema.\n",
    "\n",
    "### Example Using Python\n",
    "\n",
    "Here’s a step-by-step guide using Python, `jsonschema`, and `pyyaml` libraries:\n",
    "\n",
    "1. **Install Required Libraries**:\n",
    "   ```bash\n",
    "   pip install jsonschema pyyaml\n",
    "   ```\n",
    "\n",
    "2. **Load and Parse YAML Files**:\n",
    "   ```python\n",
    "   import yaml\n",
    "   import json\n",
    "   from jsonschema import validate, RefResolver, ValidationError\n",
    "\n",
    "   # Load the bundled JSON file containing multiple YAML schemas\n",
    "   with open('bundled.json', 'r') as bundled_file:\n",
    "       bundled_schemas = json.load(bundled_file)\n",
    "\n",
    "   # Extract and parse the _definitions.yaml file\n",
    "   definitions_yaml = bundled_schemas['_definitions.yaml']\n",
    "   definitions_schema = yaml.safe_load(definitions_yaml)\n",
    "\n",
    "   # Extract and parse the specific schema that references _definitions.yaml\n",
    "   specific_schema_yaml = bundled_schemas['specific_schema_key']  # Replace with the actual key\n",
    "   specific_schema = yaml.safe_load(specific_schema_yaml)\n",
    "   ```\n",
    "\n",
    "3. **Resolve References**:\n",
    "   ```python\n",
    "   # Create a resolver that includes the definitions\n",
    "   class CustomRefResolver(RefResolver):\n",
    "       def resolve_remote(self, uri):\n",
    "           if uri == 'definitions.yaml':\n",
    "               return definitions_schema\n",
    "           return super().resolve_remote(uri)\n",
    "\n",
    "   resolver = CustomRefResolver.from_schema(specific_schema)\n",
    "   ```\n",
    "\n",
    "4. **Validate the JSON Data**:\n",
    "   ```python\n",
    "   # Load the metadata JSON file\n",
    "   with open('metadata.json', 'r') as json_file:\n",
    "       metadata = json.load(json_file)\n",
    "\n",
    "   # Validate the metadata against the specific schema with resolved references\n",
    "   try:\n",
    "       validate(instance=metadata, schema=specific_schema, resolver=resolver)\n",
    "       print(\"Validation successful!\")\n",
    "   except ValidationError as e:\n",
    "       print(f\"Validation error: {e.message}\")\n",
    "   ```\n",
    "\n",
    "### Detailed Explanation\n",
    "\n",
    "1. **Load and Parse YAML Files**:\n",
    "   - Load the `bundled.json` file, which contains multiple YAML schemas.\n",
    "   - Extract and parse the `_definitions.yaml` file and the specific schema that references it using `yaml.safe_load`.\n",
    "\n",
    "2. **Resolve References**:\n",
    "   - Create a custom `RefResolver` class that overrides the `resolve_remote` method to return the parsed `_definitions.yaml` schema when the reference URI matches.\n",
    "   - Instantiate the custom resolver with the specific schema.\n",
    "\n",
    "3. **Validate the JSON Data**:\n",
    "   - Load the `metadata.json` file.\n",
    "   - Use the `validate` function from the `jsonschema` library to validate the `metadata.json` data against the specific schema, using the custom resolver to handle references.\n",
    "\n",
    "### Additional Resources\n",
    "\n",
    "- **GitHub - Schema Validator**: Provides a utility for validating YAML/JSON files against predefined schemas, including handling nested properties and command-line usage [1].\n",
    "- **JSON Schema - Getting Started**: Offers a comprehensive guide on creating and using JSON Schema, including defining properties, nesting data structures, and validating JSON data [2].\n",
    "- **MuleSoft Documentation**: Describes how to use the JSON Schema validator to evaluate JSON payloads at runtime, supporting both local and external schemas [4].\n",
    "- **Python JSON Schema Documentation**: Explains how to use the `jsonschema` library in Python to validate JSON documents, including handling references and custom resolvers [10][11][14].\n",
    "\n",
    "By following these steps, you can ensure that your `metadata.json` file is validated against the correct schema, even if it relies on external definitions in `_definitions.yaml`. This approach leverages Python's `jsonschema` library and custom reference resolution to handle complex schema validation scenarios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manual reference resolution\n",
    "Try to resolve the references manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function resolves the definition file\n",
    "- next steps are to use the resolved definition file to manually update the references in the target yaml to be resolved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import copy\n",
    "from jsonschema import validate, RefResolver, ValidationError\n",
    "\n",
    "# Load and parse schemas\n",
    "def load_and_parse_schemas(schema_path):\n",
    "    if not schema_path.endswith('.json'):\n",
    "        raise ValueError(\"schema_path must be a .json file\")\n",
    "\n",
    "    try:\n",
    "        with open(schema_path, 'r') as schema_file:\n",
    "            bundled_schemas = json.load(schema_file)\n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        raise ValueError(f\"Error loading JSON file: {e}\")\n",
    "\n",
    "    loaded_yamls = {}\n",
    "    for key, value in bundled_schemas.items():\n",
    "        if isinstance(value, dict):\n",
    "            try:\n",
    "                yaml_str = yaml.dump(value, sort_keys=False)\n",
    "                loaded_yamls[key] = yaml.safe_load(yaml_str)\n",
    "            except yaml.YAMLError as e:\n",
    "                raise ValueError(f\"Error parsing YAML for key '{key}': {e}\")\n",
    "        else:\n",
    "            raise ValueError(f\"Value for key '{key}' is not a dictionary\")\n",
    "\n",
    "    return loaded_yamls\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "\n",
    "def oneStepResolver(baseDir: str, baseYaml: str, refYaml: str = None, refFileObj: str = None): \n",
    "    # Construct the absolute path for the base YAML file\n",
    "    baseFilePath = os.path.join(baseDir, baseYaml)\n",
    "    \n",
    "    # Load the YAML files\n",
    "    with open(baseFilePath, 'r') as base_file:\n",
    "        baseFile = yaml.safe_load(base_file)\n",
    "    \n",
    "    if refFileObj is not None:\n",
    "        referenceFile = refFileObj\n",
    "    elif refYaml is not None:\n",
    "        refFilePath = os.path.join(baseDir, refYaml)\n",
    "        with open(refFilePath, 'r') as ref_file:\n",
    "            referenceFile = yaml.safe_load(ref_file)\n",
    "    else:\n",
    "        referenceFile = None\n",
    "    \n",
    "    # Function to resolve $ref\n",
    "    def resolve_ref(ref, referenceFile, baseFile):\n",
    "        if ref.startswith('#/'):\n",
    "            path = ref.split('#/')[1].split('/')\n",
    "            value = baseFile\n",
    "        else:\n",
    "            file_path, ref_path = ref.split('#/')\n",
    "            file_path = os.path.join(baseDir, file_path)\n",
    "            with open(file_path, 'r') as ref_file:\n",
    "                value = yaml.safe_load(ref_file)\n",
    "            path = ref_path.split('/')\n",
    "        \n",
    "        for p in path:\n",
    "            value = value[p]\n",
    "        return value\n",
    "\n",
    "    # Recursive function to resolve all $ref in a dictionary\n",
    "    def resolve_all_refs(obj, referenceFile, baseFile):\n",
    "        if isinstance(obj, dict):\n",
    "            if '$ref' in obj:\n",
    "                ref = obj['$ref']\n",
    "                resolved_value = resolve_ref(ref, referenceFile, baseFile)\n",
    "                return resolve_all_refs(resolved_value, referenceFile, baseFile)\n",
    "            else:\n",
    "                return {k: resolve_all_refs(v, referenceFile, baseFile) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [resolve_all_refs(item, referenceFile, baseFile) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Resolve the $ref in baseFile\n",
    "    resolved_baseFile = resolve_all_refs(baseFile, referenceFile, baseFile)\n",
    "\n",
    "    # Return the resolved baseFile\n",
    "    return yaml.dump(resolved_baseFile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id: _definitions\n",
      "UUID:\n",
      "  term:\n",
      "    description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "      it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\n",
      "      until 3400 AD or extremely likely to be different. Its relatively small size\n",
      "      lends itself well to sorting, ordering, and hashing of all sorts, storing in\n",
      "      databases, simple allocation, and ease of programming in general.\n",
      "\n",
      "      '\n",
      "    termDef:\n",
      "      term: Universally Unique Identifier\n",
      "      source: NCIt\n",
      "      cde_id: C54100\n",
      "      cde_version: null\n",
      "      term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "  type: string\n",
      "  pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "parent_uuids:\n",
      "  type: array\n",
      "  minItems: 1\n",
      "  items:\n",
      "    term:\n",
      "      description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "        it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\n",
      "        until 3400 AD or extremely likely to be different. Its relatively small size\n",
      "        lends itself well to sorting, ordering, and hashing of all sorts, storing\n",
      "        in databases, simple allocation, and ease of programming in general.\n",
      "\n",
      "        '\n",
      "      termDef:\n",
      "        term: Universally Unique Identifier\n",
      "        source: NCIt\n",
      "        cde_id: C54100\n",
      "        cde_version: null\n",
      "        term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "    type: string\n",
      "    pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "  uniqueItems: true\n",
      "foreign_key_project:\n",
      "  type: object\n",
      "  additionalProperties: true\n",
      "  properties:\n",
      "    id:\n",
      "      term:\n",
      "        description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "          it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\n",
      "          until 3400 AD or extremely likely to be different. Its relatively small\n",
      "          size lends itself well to sorting, ordering, and hashing of all sorts, storing\n",
      "          in databases, simple allocation, and ease of programming in general.\n",
      "\n",
      "          '\n",
      "        termDef:\n",
      "          term: Universally Unique Identifier\n",
      "          source: NCIt\n",
      "          cde_id: C54100\n",
      "          cde_version: null\n",
      "          term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "      type: string\n",
      "      pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "    code:\n",
      "      type: string\n",
      "to_one_project:\n",
      "  anyOf:\n",
      "  - type: array\n",
      "    items:\n",
      "      type: object\n",
      "      additionalProperties: true\n",
      "      properties:\n",
      "        id:\n",
      "          term:\n",
      "            description: 'A 128-bit identifier. Depending on the mechanism used to\n",
      "              generate it, it is either guaranteed to be different from all other\n",
      "              UUIDs/GUIDs generated until 3400 AD or extremely likely to be different.\n",
      "              Its relatively small size lends itself well to sorting, ordering, and\n",
      "              hashing of all sorts, storing in databases, simple allocation, and ease\n",
      "              of programming in general.\n",
      "\n",
      "              '\n",
      "            termDef:\n",
      "              term: Universally Unique Identifier\n",
      "              source: NCIt\n",
      "              cde_id: C54100\n",
      "              cde_version: null\n",
      "              term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "          type: string\n",
      "          pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "        code:\n",
      "          type: string\n",
      "  - type: object\n",
      "    additionalProperties: true\n",
      "    properties:\n",
      "      id:\n",
      "        term:\n",
      "          description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "            it, it is either guaranteed to be different from all other UUIDs/GUIDs\n",
      "            generated until 3400 AD or extremely likely to be different. Its relatively\n",
      "            small size lends itself well to sorting, ordering, and hashing of all\n",
      "            sorts, storing in databases, simple allocation, and ease of programming\n",
      "            in general.\n",
      "\n",
      "            '\n",
      "          termDef:\n",
      "            term: Universally Unique Identifier\n",
      "            source: NCIt\n",
      "            cde_id: C54100\n",
      "            cde_version: null\n",
      "            term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "        type: string\n",
      "        pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "      code:\n",
      "        type: string\n",
      "to_many_project:\n",
      "  anyOf:\n",
      "  - type: array\n",
      "    items:\n",
      "      type: object\n",
      "      additionalProperties: true\n",
      "      properties:\n",
      "        id:\n",
      "          term:\n",
      "            description: 'A 128-bit identifier. Depending on the mechanism used to\n",
      "              generate it, it is either guaranteed to be different from all other\n",
      "              UUIDs/GUIDs generated until 3400 AD or extremely likely to be different.\n",
      "              Its relatively small size lends itself well to sorting, ordering, and\n",
      "              hashing of all sorts, storing in databases, simple allocation, and ease\n",
      "              of programming in general.\n",
      "\n",
      "              '\n",
      "            termDef:\n",
      "              term: Universally Unique Identifier\n",
      "              source: NCIt\n",
      "              cde_id: C54100\n",
      "              cde_version: null\n",
      "              term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "          type: string\n",
      "          pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "        code:\n",
      "          type: string\n",
      "  - type: object\n",
      "    additionalProperties: true\n",
      "    properties:\n",
      "      id:\n",
      "        term:\n",
      "          description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "            it, it is either guaranteed to be different from all other UUIDs/GUIDs\n",
      "            generated until 3400 AD or extremely likely to be different. Its relatively\n",
      "            small size lends itself well to sorting, ordering, and hashing of all\n",
      "            sorts, storing in databases, simple allocation, and ease of programming\n",
      "            in general.\n",
      "\n",
      "            '\n",
      "          termDef:\n",
      "            term: Universally Unique Identifier\n",
      "            source: NCIt\n",
      "            cde_id: C54100\n",
      "            cde_version: null\n",
      "            term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "        type: string\n",
      "        pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "      code:\n",
      "        type: string\n",
      "foreign_key:\n",
      "  type: object\n",
      "  additionalProperties: true\n",
      "  properties:\n",
      "    id:\n",
      "      term:\n",
      "        description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "          it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\n",
      "          until 3400 AD or extremely likely to be different. Its relatively small\n",
      "          size lends itself well to sorting, ordering, and hashing of all sorts, storing\n",
      "          in databases, simple allocation, and ease of programming in general.\n",
      "\n",
      "          '\n",
      "        termDef:\n",
      "          term: Universally Unique Identifier\n",
      "          source: NCIt\n",
      "          cde_id: C54100\n",
      "          cde_version: null\n",
      "          term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "      type: string\n",
      "      pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "    submitter_id:\n",
      "      type: string\n",
      "to_one:\n",
      "  anyOf:\n",
      "  - type: array\n",
      "    items:\n",
      "      type: object\n",
      "      additionalProperties: true\n",
      "      properties:\n",
      "        id:\n",
      "          term:\n",
      "            description: 'A 128-bit identifier. Depending on the mechanism used to\n",
      "              generate it, it is either guaranteed to be different from all other\n",
      "              UUIDs/GUIDs generated until 3400 AD or extremely likely to be different.\n",
      "              Its relatively small size lends itself well to sorting, ordering, and\n",
      "              hashing of all sorts, storing in databases, simple allocation, and ease\n",
      "              of programming in general.\n",
      "\n",
      "              '\n",
      "            termDef:\n",
      "              term: Universally Unique Identifier\n",
      "              source: NCIt\n",
      "              cde_id: C54100\n",
      "              cde_version: null\n",
      "              term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "          type: string\n",
      "          pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "        submitter_id:\n",
      "          type: string\n",
      "  - type: object\n",
      "    additionalProperties: true\n",
      "    properties:\n",
      "      id:\n",
      "        term:\n",
      "          description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "            it, it is either guaranteed to be different from all other UUIDs/GUIDs\n",
      "            generated until 3400 AD or extremely likely to be different. Its relatively\n",
      "            small size lends itself well to sorting, ordering, and hashing of all\n",
      "            sorts, storing in databases, simple allocation, and ease of programming\n",
      "            in general.\n",
      "\n",
      "            '\n",
      "          termDef:\n",
      "            term: Universally Unique Identifier\n",
      "            source: NCIt\n",
      "            cde_id: C54100\n",
      "            cde_version: null\n",
      "            term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "        type: string\n",
      "        pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "      submitter_id:\n",
      "        type: string\n",
      "to_many:\n",
      "  anyOf:\n",
      "  - type: array\n",
      "    items:\n",
      "      type: object\n",
      "      additionalProperties: true\n",
      "      properties:\n",
      "        id:\n",
      "          term:\n",
      "            description: 'A 128-bit identifier. Depending on the mechanism used to\n",
      "              generate it, it is either guaranteed to be different from all other\n",
      "              UUIDs/GUIDs generated until 3400 AD or extremely likely to be different.\n",
      "              Its relatively small size lends itself well to sorting, ordering, and\n",
      "              hashing of all sorts, storing in databases, simple allocation, and ease\n",
      "              of programming in general.\n",
      "\n",
      "              '\n",
      "            termDef:\n",
      "              term: Universally Unique Identifier\n",
      "              source: NCIt\n",
      "              cde_id: C54100\n",
      "              cde_version: null\n",
      "              term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "          type: string\n",
      "          pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "        submitter_id:\n",
      "          type: string\n",
      "  - type: object\n",
      "    additionalProperties: true\n",
      "    properties:\n",
      "      id:\n",
      "        term:\n",
      "          description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "            it, it is either guaranteed to be different from all other UUIDs/GUIDs\n",
      "            generated until 3400 AD or extremely likely to be different. Its relatively\n",
      "            small size lends itself well to sorting, ordering, and hashing of all\n",
      "            sorts, storing in databases, simple allocation, and ease of programming\n",
      "            in general.\n",
      "\n",
      "            '\n",
      "          termDef:\n",
      "            term: Universally Unique Identifier\n",
      "            source: NCIt\n",
      "            cde_id: C54100\n",
      "            cde_version: null\n",
      "            term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "        type: string\n",
      "        pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "      submitter_id:\n",
      "        type: string\n",
      "datetime:\n",
      "  oneOf:\n",
      "  - type: string\n",
      "    format: date-time\n",
      "  - type: 'null'\n",
      "  term:\n",
      "    description: 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\n",
      "\n",
      "      '\n",
      "file_name:\n",
      "  type: string\n",
      "  term:\n",
      "    description: 'The name (or part of a name) of a file (of any type).\n",
      "\n",
      "      '\n",
      "file_size:\n",
      "  type: integer\n",
      "  term:\n",
      "    description: 'The size of the data file (object) in bytes.\n",
      "\n",
      "      '\n",
      "file_format:\n",
      "  type: string\n",
      "  term:\n",
      "    description: 'The format of the data file object.\n",
      "\n",
      "      '\n",
      "ga4gh_drs_uri:\n",
      "  type: string\n",
      "  term:\n",
      "    description: 'DRS URI as defined by GA4GH DRS spec for pointers to file objects.\n",
      "\n",
      "      '\n",
      "md5sum:\n",
      "  type: string\n",
      "  term:\n",
      "    description: 'The 128-bit hash value expressed as a 32 digit hexadecimal number\n",
      "      used as a file''s digital fingerprint.\n",
      "\n",
      "      '\n",
      "  pattern: ^[a-f0-9]{32}$\n",
      "object_id:\n",
      "  type: string\n",
      "  description: The GUID of the object in the index service.\n",
      "release_state:\n",
      "  description: Release state of an entity.\n",
      "  default: unreleased\n",
      "  enum:\n",
      "  - unreleased\n",
      "  - released\n",
      "  - redacted\n",
      "data_bundle_state:\n",
      "  description: State of a data bundle.\n",
      "  default: submitted\n",
      "  enum:\n",
      "  - submitted\n",
      "  - validated\n",
      "  - error\n",
      "  - released\n",
      "  - suppressed\n",
      "  - redacted\n",
      "data_file_error_type:\n",
      "  term:\n",
      "    description: 'Type of error for the data file object.\n",
      "\n",
      "      '\n",
      "  enum:\n",
      "  - file_size\n",
      "  - file_format\n",
      "  - md5sum\n",
      "state:\n",
      "  term:\n",
      "    description: 'The current state of the object.\n",
      "\n",
      "      '\n",
      "  default: validated\n",
      "  downloadable:\n",
      "  - uploaded\n",
      "  - md5summed\n",
      "  - validating\n",
      "  - validated\n",
      "  - error\n",
      "  - invalid\n",
      "  - released\n",
      "  public:\n",
      "  - live\n",
      "  oneOf:\n",
      "  - enum:\n",
      "    - uploading\n",
      "    - uploaded\n",
      "    - md5summing\n",
      "    - md5summed\n",
      "    - validating\n",
      "    - error\n",
      "    - invalid\n",
      "    - suppressed\n",
      "    - redacted\n",
      "    - live\n",
      "  - enum:\n",
      "    - validated\n",
      "    - submitted\n",
      "    - released\n",
      "file_state:\n",
      "  term:\n",
      "    description: 'The current state of the data file object.\n",
      "\n",
      "      '\n",
      "  default: registered\n",
      "  enum:\n",
      "  - registered\n",
      "  - uploading\n",
      "  - uploaded\n",
      "  - validating\n",
      "  - validated\n",
      "  - submitted\n",
      "  - processing\n",
      "  - processed\n",
      "  - released\n",
      "  - error\n",
      "qc_metrics_state:\n",
      "  term:\n",
      "    description: 'State classification given by FASTQC for the metric. Metric specific\n",
      "      details about the states are available on their website.\n",
      "\n",
      "      '\n",
      "    termDef:\n",
      "      term: QC Metric State\n",
      "      source: FastQC\n",
      "      cde_id: null\n",
      "      cde_version: null\n",
      "      term_url: http://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/\n",
      "  enum:\n",
      "  - FAIL\n",
      "  - PASS\n",
      "  - WARN\n",
      "project_id:\n",
      "  type: string\n",
      "  term:\n",
      "    description: 'Unique ID for any specific defined piece of work that is undertaken\n",
      "      or attempted to meet a single requirement.\n",
      "\n",
      "      '\n",
      "data_file_properties:\n",
      "  type:\n",
      "    type: string\n",
      "  id:\n",
      "    term:\n",
      "      description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "        it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\n",
      "        until 3400 AD or extremely likely to be different. Its relatively small size\n",
      "        lends itself well to sorting, ordering, and hashing of all sorts, storing\n",
      "        in databases, simple allocation, and ease of programming in general.\n",
      "\n",
      "        '\n",
      "      termDef:\n",
      "        term: Universally Unique Identifier\n",
      "        source: NCIt\n",
      "        cde_id: C54100\n",
      "        cde_version: null\n",
      "        term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "    type: string\n",
      "    pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "  submitter_id:\n",
      "    type:\n",
      "    - string\n",
      "    description: 'A project-specific identifier for a node. This property is the calling\n",
      "      card/nickname/alias for a unit of submission. It can be used in place of the\n",
      "      UUID for identifying or recalling a node.\n",
      "\n",
      "      '\n",
      "  state:\n",
      "    term:\n",
      "      description: 'The current state of the object.\n",
      "\n",
      "        '\n",
      "    default: validated\n",
      "    downloadable:\n",
      "    - uploaded\n",
      "    - md5summed\n",
      "    - validating\n",
      "    - validated\n",
      "    - error\n",
      "    - invalid\n",
      "    - released\n",
      "    public:\n",
      "    - live\n",
      "    oneOf:\n",
      "    - enum:\n",
      "      - uploading\n",
      "      - uploaded\n",
      "      - md5summing\n",
      "      - md5summed\n",
      "      - validating\n",
      "      - error\n",
      "      - invalid\n",
      "      - suppressed\n",
      "      - redacted\n",
      "      - live\n",
      "    - enum:\n",
      "      - validated\n",
      "      - submitted\n",
      "      - released\n",
      "  project_id:\n",
      "    type: string\n",
      "    term:\n",
      "      description: 'Unique ID for any specific defined piece of work that is undertaken\n",
      "        or attempted to meet a single requirement.\n",
      "\n",
      "        '\n",
      "  created_datetime:\n",
      "    oneOf:\n",
      "    - type: string\n",
      "      format: date-time\n",
      "    - type: 'null'\n",
      "    term:\n",
      "      description: 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\n",
      "\n",
      "        '\n",
      "  updated_datetime:\n",
      "    oneOf:\n",
      "    - type: string\n",
      "      format: date-time\n",
      "    - type: 'null'\n",
      "    term:\n",
      "      description: 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\n",
      "\n",
      "        '\n",
      "workflow_properties:\n",
      "  type:\n",
      "    type: string\n",
      "  id:\n",
      "    term:\n",
      "      description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "        it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\n",
      "        until 3400 AD or extremely likely to be different. Its relatively small size\n",
      "        lends itself well to sorting, ordering, and hashing of all sorts, storing\n",
      "        in databases, simple allocation, and ease of programming in general.\n",
      "\n",
      "        '\n",
      "      termDef:\n",
      "        term: Universally Unique Identifier\n",
      "        source: NCIt\n",
      "        cde_id: C54100\n",
      "        cde_version: null\n",
      "        term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "    type: string\n",
      "    pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "  submitter_id:\n",
      "    type:\n",
      "    - string\n",
      "    description: 'A project-specific identifier for a node. This property is the calling\n",
      "      card/nickname/alias for a unit of submission. It can be used in place of the\n",
      "      UUID for identifying or recalling a node.\n",
      "\n",
      "      '\n",
      "  state:\n",
      "    term:\n",
      "      description: 'The current state of the object.\n",
      "\n",
      "        '\n",
      "    default: validated\n",
      "    downloadable:\n",
      "    - uploaded\n",
      "    - md5summed\n",
      "    - validating\n",
      "    - validated\n",
      "    - error\n",
      "    - invalid\n",
      "    - released\n",
      "    public:\n",
      "    - live\n",
      "    oneOf:\n",
      "    - enum:\n",
      "      - uploading\n",
      "      - uploaded\n",
      "      - md5summing\n",
      "      - md5summed\n",
      "      - validating\n",
      "      - error\n",
      "      - invalid\n",
      "      - suppressed\n",
      "      - redacted\n",
      "      - live\n",
      "    - enum:\n",
      "      - validated\n",
      "      - submitted\n",
      "      - released\n",
      "  project_id:\n",
      "    type: string\n",
      "    term:\n",
      "      description: 'Unique ID for any specific defined piece of work that is undertaken\n",
      "        or attempted to meet a single requirement.\n",
      "\n",
      "        '\n",
      "  created_datetime:\n",
      "    oneOf:\n",
      "    - type: string\n",
      "      format: date-time\n",
      "    - type: 'null'\n",
      "    term:\n",
      "      description: 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\n",
      "\n",
      "        '\n",
      "  updated_datetime:\n",
      "    oneOf:\n",
      "    - type: string\n",
      "      format: date-time\n",
      "    - type: 'null'\n",
      "    term:\n",
      "      description: 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\n",
      "\n",
      "        '\n",
      "ubiquitous_properties:\n",
      "  type:\n",
      "    type: string\n",
      "  id:\n",
      "    term:\n",
      "      description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "        it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\n",
      "        until 3400 AD or extremely likely to be different. Its relatively small size\n",
      "        lends itself well to sorting, ordering, and hashing of all sorts, storing\n",
      "        in databases, simple allocation, and ease of programming in general.\n",
      "\n",
      "        '\n",
      "      termDef:\n",
      "        term: Universally Unique Identifier\n",
      "        source: NCIt\n",
      "        cde_id: C54100\n",
      "        cde_version: null\n",
      "        term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "    type: string\n",
      "    pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "  submitter_id:\n",
      "    type:\n",
      "    - string\n",
      "    description: 'A project-specific identifier for a node. This property is the calling\n",
      "      card/nickname/alias for a unit of submission. It can be used in place of the\n",
      "      UUID for identifying or recalling a node.\n",
      "\n",
      "      '\n",
      "  state:\n",
      "    term:\n",
      "      description: 'The current state of the object.\n",
      "\n",
      "        '\n",
      "    default: validated\n",
      "    downloadable:\n",
      "    - uploaded\n",
      "    - md5summed\n",
      "    - validating\n",
      "    - validated\n",
      "    - error\n",
      "    - invalid\n",
      "    - released\n",
      "    public:\n",
      "    - live\n",
      "    oneOf:\n",
      "    - enum:\n",
      "      - uploading\n",
      "      - uploaded\n",
      "      - md5summing\n",
      "      - md5summed\n",
      "      - validating\n",
      "      - error\n",
      "      - invalid\n",
      "      - suppressed\n",
      "      - redacted\n",
      "      - live\n",
      "    - enum:\n",
      "      - validated\n",
      "      - submitted\n",
      "      - released\n",
      "  project_id:\n",
      "    type: string\n",
      "    term:\n",
      "      description: 'Unique ID for any specific defined piece of work that is undertaken\n",
      "        or attempted to meet a single requirement.\n",
      "\n",
      "        '\n",
      "  created_datetime:\n",
      "    oneOf:\n",
      "    - type: string\n",
      "      format: date-time\n",
      "    - type: 'null'\n",
      "    term:\n",
      "      description: 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\n",
      "\n",
      "        '\n",
      "  updated_datetime:\n",
      "    oneOf:\n",
      "    - type: string\n",
      "      format: date-time\n",
      "    - type: 'null'\n",
      "    term:\n",
      "      description: 'A combination of date and time of day in the form [-]CCYY-MM-DDThh:mm:ss[Z|(+|-)hh:mm]\n",
      "\n",
      "        '\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Resolve the _definitions.yaml using the _settings.yaml\n",
    "resolved_project_yaml = oneStepResolver('/Users/harrijh/Library/CloudStorage/GoogleDrive-joshua@biocommons.org.au/My Drive/projects/ACDCSchemaDev/output/schema/yaml/', '_definitions.yaml', '_settings.yaml')\n",
    "\n",
    "# Now, resolve the project.yaml using the parsed resolved_def\n",
    "print(resolved_project_yaml)\n",
    "\n",
    "# writing\n",
    "with open('../output/schema/yaml/_definitions_res.yaml', 'w') as f:\n",
    "    f.write(resolved_project_yaml)\n",
    "\n",
    "# storing as dict\n",
    "resolved_project_yaml_dict = yaml.safe_load(resolved_project_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': '_definitions.yaml#/ubiquitous_properties', 'properties.subjects': '_definitions.yaml#/to_one'}\n"
     ]
    }
   ],
   "source": [
    "# function to use _definitions_res.yaml to resolve the target yaml\n",
    "\n",
    "# Load schemas\n",
    "schema_path = '../output/schema/json/schema_dev.json'\n",
    "yaml_dict = load_and_parse_schemas(schema_path)\n",
    "\n",
    "medYaml = yaml_dict['medical_history.yaml']\n",
    "\n",
    "# Function to pull key-value pairs where key is '$ref' and store in a dictionary\n",
    "def extract_refs(yaml_dict):\n",
    "    refs = {}\n",
    "\n",
    "    def recursive_extract(obj, parent_key=''):\n",
    "        if isinstance(obj, dict):\n",
    "            for k, v in obj.items():\n",
    "                if k == '$ref':\n",
    "                    refs[parent_key] = v\n",
    "                else:\n",
    "                    new_key = f\"{parent_key}.{k}\" if parent_key else k\n",
    "                    recursive_extract(v, new_key)\n",
    "        elif isinstance(obj, list):\n",
    "            for i, item in enumerate(obj):\n",
    "                new_key = f\"{parent_key}[{i}]\"\n",
    "                recursive_extract(item, new_key)\n",
    "\n",
    "    recursive_extract(yaml_dict)\n",
    "    return refs\n",
    "\n",
    "# Example usage\n",
    "refs_dict = extract_refs(medYaml)\n",
    "print(refs_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# class for string manipulation\n",
    "\n",
    "class refString:\n",
    "    \"\"\"\n",
    "    Attributes:\n",
    "        str_value (str): The original reference string ('_definitions.yaml#/ubiquitous_properties).\n",
    "        yamlName (str): The extracted YAML file name from the reference string (output = _definitions.yaml).\n",
    "        propName (str): The extracted property name from the reference string (output = ubiquitous_properties).\n",
    "    \"\"\"\n",
    "    def __init__(self, str_value: str):\n",
    "        self.str_value = str_value\n",
    "        ref_value = self.str_value.replace('#', '')\n",
    "        self.yamlName, self.propName = ref_value.split('/')\n",
    "    \n",
    "    def get_yaml_name(self):\n",
    "        return self.yamlName\n",
    "    \n",
    "    def get_prop_name(self):\n",
    "        return self.propName\n",
    "    \n",
    "\n",
    "# def extract_last_key(key_string):\n",
    "#     \"\"\"\n",
    "#     Extracts the last segment from each string in a list, separated by dots.\n",
    "\n",
    "#     Parameters:\n",
    "#     - strings (list): List of dot-separated strings.\n",
    "\n",
    "#     Returns:\n",
    "#     - list: List of the last segment from each string.\n",
    "\n",
    "#     Example:\n",
    "#     >>>extract_last_item([\"aye\", \"aye.bee\", \"aye.bee.ceebs\"])\n",
    "#     ['aye', 'bee', 'ceebs']\n",
    "#     \"\"\"\n",
    "#     return key_string.split('.')[-1]\n",
    "    \n",
    "\n",
    "# # pulling value base on propName \n",
    "# def get_value_by_ref(data, ref_str):\n",
    "#     \"\"\"\n",
    "#     Recursively search for the ref_str in the nested dictionary and return its value.\n",
    "    \n",
    "#     :param data: The dictionary to search within.\n",
    "#     :param ref_str: The reference string to search for.\n",
    "#     :return: The value associated with the ref_str, or None if the key is not found.\n",
    "#     \"\"\"\n",
    "#     if isinstance(data, dict):\n",
    "#         for key, value in data.items():\n",
    "#             if key == ref_str:\n",
    "#                 print('key found')\n",
    "#                 return value\n",
    "#             elif isinstance(value, (dict, list)):\n",
    "#                 result = get_value_by_ref(value, ref_str)\n",
    "#                 if result is not None:\n",
    "#                     return result\n",
    "#     elif isinstance(data, list):\n",
    "#         for item in data:\n",
    "#             result = get_value_by_ref(item, ref_str)\n",
    "#             if result is not None:\n",
    "#                 return result\n",
    "#     return None\n",
    "\n",
    "\n",
    "def get_value_by_ref(data, ref_str):\n",
    "    result = data[ref_str]\n",
    "    result_str = yaml.dump(result, default_flow_style=False, sort_keys=False)\n",
    "    return result_str\n",
    "\n",
    "\n",
    "def process_ref_value(ref_value: str, resolved_project_yaml_dict: dict):\n",
    "    \"\"\"\n",
    "    Uses the reference value to pull the property from the resolved_project_yaml_dict, and then returns the insert value string and the replace value string.\n",
    "    \n",
    "    Parameters:\n",
    "    - ref_value (str): The reference value to process ('_definitions.yaml#/to_one').\n",
    "    - resolved_project_yaml_dict (dict): The resolved definitions YAML dictionary.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple: A tuple containing the insert value string and the replace value string.\n",
    "    \"\"\"\n",
    "    \n",
    "    propName = refString(ref_value).get_prop_name()\n",
    "    yamlName = refString(ref_value).get_yaml_name()\n",
    "\n",
    "    # check that yamlName = _definitions.yaml\n",
    "    if yamlName != '_definitions.yaml':\n",
    "        print('not _definitions.yaml')\n",
    "\n",
    "    # pulling prop value from the resolved _definitions\n",
    "    prop_value = get_value_by_ref(resolved_project_yaml_dict, propName)\n",
    "    # prop_value_str = yaml.dump(prop_value, default_flow_style=False, sort_keys=False)\n",
    "    # print(f\"Value for property '{propName}': {prop_value}\")\n",
    "\n",
    "    # creating final strings or values\n",
    "    insert_value_str = f\"$ref: {ref_value}\"\n",
    "    replace_value_str = f\"{propName}: {prop_value}\"\n",
    "\n",
    "    # returning the match replace key value pair\n",
    "    return insert_value_str, replace_value_str\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# continue from here\n",
    "- Problem is that when the string is inserted, it does not insert properly and formatting is wrong. \n",
    "- Would ideally need to insert using official dictionary value replacements, but can be difficult when replacing values in nested dictionaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finding ref for _definitions.yaml#/to_one in resolved definitions\n",
      "ref value is: to_one: anyOf:\n",
      "- type: array\n",
      "  items:\n",
      "    type: object\n",
      "    additionalProperties: true\n",
      "    properties:\n",
      "      id:\n",
      "        term:\n",
      "          description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "            it, it is either guaranteed to be different from all other UUIDs/GUIDs\n",
      "            generated until 3400 AD or extremely likely to be different. Its relatively\n",
      "            small size lends itself well to sorting, ordering, and hashing of all\n",
      "            sorts, storing in databases, simple allocation, and ease of programming\n",
      "            in general.\n",
      "\n",
      "            '\n",
      "          termDef:\n",
      "            term: Universally Unique Identifier\n",
      "            source: NCIt\n",
      "            cde_id: C54100\n",
      "            cde_version: null\n",
      "            term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "        type: string\n",
      "        pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "      submitter_id:\n",
      "        type: string\n",
      "- type: object\n",
      "  additionalProperties: true\n",
      "  properties:\n",
      "    id:\n",
      "      term:\n",
      "        description: 'A 128-bit identifier. Depending on the mechanism used to generate\n",
      "          it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\n",
      "          until 3400 AD or extremely likely to be different. Its relatively small\n",
      "          size lends itself well to sorting, ordering, and hashing of all sorts, storing\n",
      "          in databases, simple allocation, and ease of programming in general.\n",
      "\n",
      "          '\n",
      "        termDef:\n",
      "          term: Universally Unique Identifier\n",
      "          source: NCIt\n",
      "          cde_id: C54100\n",
      "          cde_version: null\n",
      "          term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\n",
      "      type: string\n",
      "      pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\n",
      "    submitter_id:\n",
      "      type: string\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'$ref': '_definitions.yaml#/ubiquitous_properties',\n",
       " 'subjects': \"to_one: anyOf:\\n- type: array\\n  items:\\n    type: object\\n    additionalProperties: true\\n    properties:\\n      id:\\n        term:\\n          description: 'A 128-bit identifier. Depending on the mechanism used to generate\\n            it, it is either guaranteed to be different from all other UUIDs/GUIDs\\n            generated until 3400 AD or extremely likely to be different. Its relatively\\n            small size lends itself well to sorting, ordering, and hashing of all\\n            sorts, storing in databases, simple allocation, and ease of programming\\n            in general.\\n\\n            '\\n          termDef:\\n            term: Universally Unique Identifier\\n            source: NCIt\\n            cde_id: C54100\\n            cde_version: null\\n            term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\\n        type: string\\n        pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\\n      submitter_id:\\n        type: string\\n- type: object\\n  additionalProperties: true\\n  properties:\\n    id:\\n      term:\\n        description: 'A 128-bit identifier. Depending on the mechanism used to generate\\n          it, it is either guaranteed to be different from all other UUIDs/GUIDs generated\\n          until 3400 AD or extremely likely to be different. Its relatively small\\n          size lends itself well to sorting, ordering, and hashing of all sorts, storing\\n          in databases, simple allocation, and ease of programming in general.\\n\\n          '\\n        termDef:\\n          term: Universally Unique Identifier\\n          source: NCIt\\n          cde_id: C54100\\n          cde_version: null\\n          term_url: https://ncit.nci.nih.gov/ncitbrowser/ConceptReport.jsp?dictionary=NCI_Thesaurus&version=16.02d&ns=NCI_Thesaurus&code=C54100\\n      type: string\\n      pattern: ^[a-fA-F0-9]{8}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{4}-[a-fA-F0-9]{12}$\\n    submitter_id:\\n      type: string\\n\",\n",
       " 'hypertension': {'description': 'Whether the participant has Hypertension',\n",
       "  'enum': ['yes, measured or on treatment',\n",
       "   'yes, self-reported',\n",
       "   'no',\n",
       "   'not reported',\n",
       "   'not collected']},\n",
       " 'hypertension_measurement_type': {'description': 'Whether the hypertension was measured at recruitment/patients on hypertension treatment or self-reported medical diagnosis',\n",
       "  'enum': ['self-reported', 'measured']},\n",
       " 'diabetes': {'description': 'Self-reported diabetes in the participant',\n",
       "  'termDef': [{'term': 'Diabetes mellitus',\n",
       "    'source': 'hpo',\n",
       "    'term_id': 'HP:0000819'}],\n",
       "  'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']},\n",
       " 'diabetes_type': {'description': 'diabetes diagnosed by fasting blood glucose >=7 mmol/l or tx AHA or 2 hour blood glucose >=11.1 mmol/l.',\n",
       "  'termDef': [{'term': 'Diabetes mellitus',\n",
       "    'source': 'hpo',\n",
       "    'term_id': 'HP:0000819',\n",
       "    'term_version': '2021-10-10'}],\n",
       "  'enum': ['IGT', 'KDM', 'IFG', 'NDM', 'NGT'],\n",
       "  'enumDef': [{'enumeration': 'IGT', 'source': 'hpo', 'term_id': 'HP:0040270'},\n",
       "   {'enumeration': 'NDM', 'source': 'SNOMED', 'term_id': '870528001'},\n",
       "   {'enumeration': 'NGT', 'source': 'SNOMED', 'term_id': '166926006'}]},\n",
       " 'stroke': {'description': 'Self report of previous stroke',\n",
       "  'termDef': [{'term': 'Stroke', 'source': 'hpo', 'term_id': 'HP:0001297'}],\n",
       "  'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']},\n",
       " 'myocardial_infarction': {'description': 'self report of previous myocardial infarction (heart attack)',\n",
       "  'termDef': [{'term': 'Myocardial infarction',\n",
       "    'source': 'hpo',\n",
       "    'term_id': 'HP:0001658'}],\n",
       "  'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']},\n",
       " 'angina': {'description': 'self report of angina',\n",
       "  'termDef': [{'term': 'Angina pectoris',\n",
       "    'source': 'hpo',\n",
       "    'term_id': 'HP:0001681'}],\n",
       "  'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']},\n",
       " 'incident_diabetes': {'description': 'incident cases of diabetes ',\n",
       "  'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace_value_deep_dot_path(nested_dict, dot_path, new_value):\n",
    "    \"\"\"\n",
    "    Replace the value of a key at a specified path in a nested dictionary,\n",
    "    where the path is given as a dot-separated string.\n",
    "    \n",
    "    :param nested_dict: The nested dictionary to modify.\n",
    "    :param dot_path: A dot-separated string representing the path to the target key.\n",
    "    :param new_value: The new value to assign to the target key.\n",
    "    \"\"\"\n",
    "    path = dot_path.split('.')  # Split the dot-separated string into a list of keys\n",
    "    current = nested_dict\n",
    "    for key in path[:-1]:  # Traverse to the parent of the target key\n",
    "        if key not in current or not isinstance(current[key], dict):\n",
    "            current[key] = {}  # Ensure the key exists and is a dictionary\n",
    "        current = current[key]\n",
    "    current[path[-1]] = new_value  # Set the new value for the target key\n",
    "    return current\n",
    "\n",
    "# extracting reference values\n",
    "ref_value= list(refs_dict.values())[1]\n",
    "key_value = list(refs_dict.keys())[1]\n",
    "# convert medYaml to str (to enable replacement)\n",
    "# medYamlStr = yaml.dump(medYaml, default_flow_style=False, sort_keys=False)\n",
    "print(f'finding ref for {ref_value} in resolved definitions')\n",
    "insert_value_str, replace_value_str = process_ref_value(ref_value, resolved_project_yaml_dict)\n",
    "print(f'ref value is: {replace_value_str}')\n",
    "\n",
    "# Replacing based on key\n",
    "medYaml_edit = medYaml\n",
    "medYaml_edit = replace_value_deep_dot_path(medYaml_edit, key_value, replace_value_str)\n",
    "medYaml_edit\n",
    "# # writing medYaml to file\n",
    "# with open('../output/schema/yaml/medical_history_res.yaml', 'w') as f:\n",
    "#     f.write(medYamlStr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'medYamlStr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmedYamlStr\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'medYamlStr' is not defined"
     ]
    }
   ],
   "source": [
    "medYamlStr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is the final usage code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$schema': 'http://json-schema.org/draft-04/schema#', 'id': 'medical_history', 'title': 'Medical History', 'type': 'object', 'namespace': 'https://data.acdc.ozheart.org', 'category': 'clinical', 'program': '*', 'project': '*', 'description': 'Medical history of the participant', 'additionalProperties': False, 'submittable': True, 'validators': None, 'systemProperties': ['id', 'project_id', 'state', 'created_datetime', 'updated_datetime'], 'links': [{'name': 'subjects', 'backref': 'medical_histories', 'label': 'describes', 'target_type': 'subject', 'multiplicity': 'one_to_one', 'required': True}], 'required': ['type', 'submitter_id', 'subjects'], 'uniqueKeys': [['id'], ['project_id', 'submitter_id']], 'properties': {'$ref': '_definitions.yaml#/ubiquitous_properties', 'subjects': {'$ref': '_definitions.yaml#/to_one'}, 'hypertension': {'description': 'Whether the participant has Hypertension', 'enum': ['yes, measured or on treatment', 'yes, self-reported', 'no', 'not reported', 'not collected']}, 'hypertension_measurement_type': {'description': 'Whether the hypertension was measured at recruitment/patients on hypertension treatment or self-reported medical diagnosis', 'enum': ['self-reported', 'measured']}, 'diabetes': {'description': 'Self-reported diabetes in the participant', 'termDef': [{'term': 'Diabetes mellitus', 'source': 'hpo', 'term_id': 'HP:0000819'}], 'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']}, 'diabetes_type': {'description': 'diabetes diagnosed by fasting blood glucose >=7 mmol/l or tx AHA or 2 hour blood glucose >=11.1 mmol/l.', 'termDef': [{'term': 'Diabetes mellitus', 'source': 'hpo', 'term_id': 'HP:0000819', 'term_version': '2021-10-10'}], 'enum': ['IGT', 'KDM', 'IFG', 'NDM', 'NGT'], 'enumDef': [{'enumeration': 'IGT', 'source': 'hpo', 'term_id': 'HP:0040270'}, {'enumeration': 'NDM', 'source': 'SNOMED', 'term_id': '870528001'}, {'enumeration': 'NGT', 'source': 'SNOMED', 'term_id': '166926006'}]}, 'stroke': {'description': 'Self report of previous stroke', 'termDef': [{'term': 'Stroke', 'source': 'hpo', 'term_id': 'HP:0001297'}], 'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']}, 'myocardial_infarction': {'description': 'self report of previous myocardial infarction (heart attack)', 'termDef': [{'term': 'Myocardial infarction', 'source': 'hpo', 'term_id': 'HP:0001658'}], 'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']}, 'angina': {'description': 'self report of angina', 'termDef': [{'term': 'Angina pectoris', 'source': 'hpo', 'term_id': 'HP:0001681'}], 'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']}, 'incident_diabetes': {'description': 'incident cases of diabetes ', 'enum': ['yes', 'no', 'unsure', 'not reported', 'not collected']}}}\n",
      "finding ref for _definitions.yaml#/ubiquitous_properties in resolved definitions\n"
     ]
    }
   ],
   "source": [
    "# Load schemas\n",
    "schema_path = '../output/schema/json/schema_dev.json'\n",
    "yaml_dict = load_and_parse_schemas(schema_path)\n",
    "\n",
    "medYaml = yaml_dict['medical_history.yaml']\n",
    "print(medYaml)\n",
    "# extracting reference values\n",
    "ref_value_list = list(refs_dict.values())\n",
    "# convert medYaml to str (to enable replacement)\n",
    "medYamlStr = yaml.dump(medYaml, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "ref_value = ref_value_list[0]\n",
    "print(f'finding ref for {ref_value} in resolved definitions')\n",
    "insert_value_str, replace_value_str = process_ref_value(ref_value, resolved_project_yaml_dict)\n",
    "medYamlStr = medYamlStr.replace(insert_value_str, replace_value_str)\n",
    "    \n",
    "# medYamlStr\n",
    "# medYamlRes = yaml.safe_load(medYamlStr)\n",
    "\n",
    "\n",
    "# writing medYaml to file\n",
    "with open('../output/schema/yaml/medical_history_res.yaml', 'w') as f:\n",
    "    f.write(medYamlStr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refining the def_resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resolve_definitions(self, baseDir: str, baseYaml: str, refYaml: str = None, refFileObj: str = None): \n",
    "    # Construct the absolute path for the base YAML file\n",
    "    baseFilePath = os.path.join(baseDir, baseYaml)\n",
    "    \n",
    "    # Load the YAML files\n",
    "    with open(baseFilePath, 'r') as base_file:\n",
    "        baseFile = yaml.safe_load(base_file)\n",
    "    \n",
    "    if refFileObj is not None:\n",
    "        referenceFile = refFileObj\n",
    "    elif refYaml is not None:\n",
    "        refFilePath = os.path.join(baseDir, refYaml)\n",
    "        with open(refFilePath, 'r') as ref_file:\n",
    "            referenceFile = yaml.safe_load(ref_file)\n",
    "    else:\n",
    "        referenceFile = None\n",
    "    \n",
    "    # Function to resolve $ref\n",
    "    def resolve_ref(ref, referenceFile, baseFile):\n",
    "        if ref.startswith('#/'):\n",
    "            path = ref.split('#/')[1].split('/')\n",
    "            value = baseFile\n",
    "        else:\n",
    "            file_path, ref_path = ref.split('#/')\n",
    "            file_path = os.path.join(baseDir, file_path)\n",
    "            with open(file_path, 'r') as ref_file:\n",
    "                value = yaml.safe_load(ref_file)\n",
    "            path = ref_path.split('/')\n",
    "        \n",
    "        for p in path:\n",
    "            value = value[p]\n",
    "        return value\n",
    "\n",
    "    # Recursive function to resolve all $ref in a dictionary\n",
    "    def resolve_all_refs(obj, referenceFile, baseFile):\n",
    "        if isinstance(obj, dict):\n",
    "            if '$ref' in obj:\n",
    "                ref = obj['$ref']\n",
    "                resolved_value = resolve_ref(ref, referenceFile, baseFile)\n",
    "                return resolve_all_refs(resolved_value, referenceFile, baseFile)\n",
    "            else:\n",
    "                return {k: resolve_all_refs(v, referenceFile, baseFile) for k, v in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [resolve_all_refs(item, referenceFile, baseFile) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    # Resolve the $ref in baseFile\n",
    "    resolved_baseFile = resolve_all_refs(baseFile, referenceFile, baseFile)\n",
    "\n",
    "    # Return the resolved baseFile\n",
    "    return yaml.dump(resolved_baseFile, default_flow_style=False, sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lets test if my other resolver can resolve definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here are the functions for my class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing demographic.yaml to json\n",
      "writing project.yaml to json\n",
      "writing serum_marker_assay.yaml to json\n",
      "writing alignment_workflow.yaml to json\n",
      "writing lipidomics_assay.yaml to json\n",
      "writing metabolomics_file.yaml to json\n",
      "writing acknowledgement.yaml to json\n",
      "writing medical_history.yaml to json\n",
      "writing _definitions.yaml to json\n",
      "writing _settings.yaml to json\n",
      "writing blood_pressure_test.yaml to json\n",
      "writing genomics_assay.yaml to json\n",
      "writing variant_file.yaml to json\n",
      "writing program.yaml to json\n",
      "writing lipidomics_mapping.yaml to json\n",
      "writing serum_marker_file.yaml to json\n",
      "writing proteomics_assay.yaml to json\n",
      "writing sample.yaml to json\n",
      "writing unaligned_reads_file.yaml to json\n",
      "writing _terms.yaml to json\n",
      "writing aligned_reads_index_file.yaml to json\n",
      "writing variant_workflow.yaml to json\n",
      "writing proteomics_file.yaml to json\n",
      "writing exposure.yaml to json\n",
      "writing metabolomics_assay.yaml to json\n",
      "writing lipidomics_file.yaml to json\n",
      "writing aligned_reads_file.yaml to json\n",
      "writing lab_result.yaml to json\n",
      "writing medication.yaml to json\n",
      "writing publication.yaml to json\n",
      "writing subject.yaml to json\n",
      "writing core_metadata_collection.yaml to json\n",
      "../output/schema/json/unresolved/demographic.json successfully saved\n",
      "../output/schema/json/unresolved/project.json successfully saved\n",
      "../output/schema/json/unresolved/serum_marker_assay.json successfully saved\n",
      "../output/schema/json/unresolved/alignment_workflow.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/metabolomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/acknowledgement.json successfully saved\n",
      "../output/schema/json/unresolved/medical_history.json successfully saved\n",
      "../output/schema/json/unresolved/_definitions.json successfully saved\n",
      "../output/schema/json/unresolved/_settings.json successfully saved\n",
      "../output/schema/json/unresolved/blood_pressure_test.json successfully saved\n",
      "../output/schema/json/unresolved/genomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/variant_file.json successfully saved\n",
      "../output/schema/json/unresolved/program.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_mapping.json successfully saved\n",
      "../output/schema/json/unresolved/serum_marker_file.json successfully saved\n",
      "../output/schema/json/unresolved/proteomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/sample.json successfully saved\n",
      "../output/schema/json/unresolved/unaligned_reads_file.json successfully saved\n",
      "../output/schema/json/unresolved/_terms.json successfully saved\n",
      "../output/schema/json/unresolved/aligned_reads_index_file.json successfully saved\n",
      "../output/schema/json/unresolved/variant_workflow.json successfully saved\n",
      "../output/schema/json/unresolved/proteomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/exposure.json successfully saved\n",
      "../output/schema/json/unresolved/metabolomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/aligned_reads_file.json successfully saved\n",
      "../output/schema/json/unresolved/lab_result.json successfully saved\n",
      "../output/schema/json/unresolved/medication.json successfully saved\n",
      "../output/schema/json/unresolved/publication.json successfully saved\n",
      "../output/schema/json/unresolved/subject.json successfully saved\n",
      "../output/schema/json/unresolved/core_metadata_collection.json successfully saved\n"
     ]
    }
   ],
   "source": [
    "# loading bundled jsons and saving them as indvidual\n",
    "\n",
    "# importing directory creation from os library\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def splitBundleJson(bundleJsonPath: str, writeDir: str, returnDict: bool = False):\n",
    "    # opening bundle json\n",
    "    with open(bundleJsonPath, 'r') as f:\n",
    "        bundleJson = json.load(f)\n",
    "    bundleJsonKeys = list(bundleJson.keys())\n",
    "    \n",
    "    # saving indvidual jsons\n",
    "    outJsons = {}\n",
    "    for bundleKey in bundleJsonKeys:\n",
    "        print(f'writing {bundleKey} to json')\n",
    "        keyName = bundleKey.replace('.yaml', '.json')\n",
    "        outJsons[keyName] = bundleJson[bundleKey]\n",
    "    \n",
    "    # writing indvidual jsons\n",
    "    for keys in outJsons.keys():\n",
    "        outJsonPath = os.path.join(writeDir, keys)\n",
    "        with open(outJsonPath, 'w') as f:\n",
    "            json.dump(outJsons[keys], f)\n",
    "        print(f'{outJsonPath} successfully saved')\n",
    "    \n",
    "    if returnDict:\n",
    "        return outJsons\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def read_json(base_path: str, json_fn: str):\n",
    "    json_path = os.path.join(base_path, json_fn)\n",
    "    with open(json_path, 'r') as f:\n",
    "        schema = json.load(f)\n",
    "    print(f'{json_path} successfully loaded')\n",
    "    return schema\n",
    "\n",
    "def write_json(base_path: str, json_fn: str, schema: dict):\n",
    "    json_path = os.path.join(base_path, json_fn)\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(schema, f, indent=4)\n",
    "    print(f'{json_path} successfully saved')\n",
    "\n",
    "def resolve_references(schema, base_path):\n",
    "    def resolve_node(node):\n",
    "        if isinstance(node, dict):\n",
    "            if '$ref' in node:\n",
    "                ref_path = node['$ref']\n",
    "                ref_file, ref_key = ref_path.split('#')\n",
    "                ref_file = ref_file.strip()\n",
    "                ref_key = ref_key.strip('/')\n",
    "\n",
    "                if ref_file:\n",
    "                    ref_file_path = os.path.join(base_path, ref_file)\n",
    "                    with open(ref_file_path, 'r') as f:\n",
    "                        ref_content = json.load(f)\n",
    "                else:\n",
    "                    ref_content = schema\n",
    "\n",
    "                for part in ref_key.split('/'):\n",
    "                    ref_content = ref_content[part]\n",
    "\n",
    "                resolved_content = resolve_node(ref_content)\n",
    "                # Merge resolved content with the current node, excluding the $ref key\n",
    "                return {**resolved_content, **{k: resolve_node(v) for k, v in node.items() if k != '$ref'}}\n",
    "            else:\n",
    "                return {k: resolve_node(v) for k, v in node.items()}\n",
    "        elif isinstance(node, list):\n",
    "            return [resolve_node(item) for item in node]\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    return resolve_node(schema)\n",
    "\n",
    "def redefine_ref_path(match_str: str, replace_str: str, schema: dict):\n",
    "    \"\"\"\n",
    "    Recursively replaces $ref paths in a JSON schema.\n",
    "\n",
    "    Args:\n",
    "    - match_str (str): The string to match in the $ref path.\n",
    "    - replace_str (str): The string to replace the match_str with.\n",
    "    - schema (dict): The JSON schema to modify.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The modified JSON schema with updated $ref paths.\n",
    "    \"\"\"\n",
    "    def replace_refs(node):\n",
    "        if isinstance(node, dict):\n",
    "            if '$ref' in node and match_str in node['$ref']:\n",
    "                node['$ref'] = node['$ref'].replace(match_str, replace_str)\n",
    "            return {k: replace_refs(v) for k, v in node.items()}\n",
    "        elif isinstance(node, list):\n",
    "            return [replace_refs(item) for item in node]\n",
    "        else:\n",
    "            return node\n",
    "\n",
    "    return replace_refs(schema)\n",
    "\n",
    "\n",
    "def resolve_refs(schema_fn, ref_fn, base_path):\n",
    "    \"\"\"\n",
    "    Resolves references in a JSON schema file using definitions from another JSON file.\n",
    "\n",
    "    Args:\n",
    "    - schema_file (str): The name of the schema JSON file to resolve.\n",
    "    - ref_file (str): The name of the  JSON file with the reference definitions.\n",
    "    - base_path (str): The base directory where the schema and definition files are located.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The resolved JSON schema.\n",
    "    \"\"\"\n",
    "    # Read JSON files\n",
    "    schema_obj = read_json(base_path, schema_fn)\n",
    "    ref_obj = read_json('.', ref_fn)\n",
    "\n",
    "    # Redefine $ref paths in schema_obj if necessary\n",
    "    if '_definitions.json' in schema_fn:\n",
    "        schema_obj = redefine_ref_path('_terms.yaml', '_terms.json', schema_obj)\n",
    "    else:\n",
    "        schema_obj = redefine_ref_path('_definitions.yaml', '_definitions_[resolved].json', schema_obj)\n",
    "        schema_obj = redefine_ref_path('.yaml', '.json', schema_obj)\n",
    "\n",
    "    # Resolve references\n",
    "    resolved_schema = resolve_references(schema_obj, base_path)\n",
    "    \n",
    "    # Write resolved schema\n",
    "    resolved_schema_file = schema_fn.replace('.json', '_[resolved].json')\n",
    "    # resolved_dir = os.path.join(base_path, '../resolved')\n",
    "    # os.makedirs(base_path, exist_ok=True)\n",
    "    write_json(base_path, resolved_schema_file, resolved_schema)\n",
    "\n",
    "    if resolved_schema is not None:\n",
    "        return print(f'=== {schema_fn} successfully resolved ===')\n",
    "    else:\n",
    "        return print(f'=== {schema_fn} failed to resolve ===')\n",
    "\n",
    "def move_resolved_schemas(base_dir, target_dir):\n",
    "    \"\"\"\n",
    "    Moves resolved schemas to a new directory.\n",
    "\n",
    "    Args:\n",
    "    - base_dir (str): The base directory where the resolved schemas are located.\n",
    "    - target_dir (str): The target directory where the resolved schemas should be moved.\n",
    "    \"\"\"\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "    ref_files = [f for f in os.listdir(base_dir) if '[resolved].json' in f]\n",
    "    for f in ref_files:\n",
    "        shutil.move(os.path.join(base_dir, f), os.path.join(target_dir, f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if this path exists, execute rmtree:\n",
    "output_dir = '../output/schema/json/unresolved'\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# splitting jsons\n",
    "splitBundleJson('../output/schema/json/schema_dev.json', output_dir)\n",
    "\n",
    "# Resolving master definition file\n",
    "baseDir = '../output/schema/json/unresolved'\n",
    "refDir = os.path.join(baseDir, '_terms.json')\n",
    "def_resolved_schema = resolve_refs('_definitions.json', refDir, baseDir)\n",
    "\n",
    "# Resolving all other json schemas\n",
    "jsonfn = os.listdir(baseDir)\n",
    "jsonfn = [fn for fn in jsonfn if not fn.startswith('_')] # excluding any filename starting with _\n",
    "refDir = os.path.join(baseDir, '_definitions_[resolved].json') # dir for resolved reference file\n",
    "for fn in jsonfn:\n",
    "    resolve_refs(fn, refDir, baseDir)\n",
    "\n",
    "# Moving schemas\n",
    "move_resolved_schemas(baseDir, '../output/schema/json/resolved')\n",
    "\n",
    "# Put Validation stuff down here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is my draft Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from jsonschema import Draft4Validator, exceptions\n",
    "import yaml\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "class SchemaValidator:\n",
    "    def __init__(self, base_path: str, bundle_json_path: str):\n",
    "        \"\"\"\n",
    "        Initializes the SchemaValidator with a base path and bundle JSON path.\n",
    "\n",
    "        Args:\n",
    "        - base_path (str): The base directory path where JSON schemas are located.\n",
    "        - bundle_json_path (str): The path to the bundle JSON file.\n",
    "        \"\"\"\n",
    "        self.base_path = base_path\n",
    "        self.bundle_json_path = bundle_json_path\n",
    "\n",
    "    def split_bundle_json(self, write_dir: str, return_dict: bool = False):\n",
    "        # opening bundle json\n",
    "        with open(self.bundle_json_path, 'r') as f:\n",
    "            bundle_json = json.load(f)\n",
    "        bundle_json_keys = list(bundle_json.keys())\n",
    "        \n",
    "        # saving individual jsons\n",
    "        out_jsons = {}\n",
    "        for bundle_key in bundle_json_keys:\n",
    "            print(f'writing {bundle_key} to json')\n",
    "            key_name = bundle_key.replace('.yaml', '.json')\n",
    "            out_jsons[key_name] = bundle_json[bundle_key]\n",
    "        \n",
    "        # writing individual jsons\n",
    "        for keys in out_jsons.keys():\n",
    "            out_json_path = os.path.join(write_dir, keys)\n",
    "            with open(out_json_path, 'w') as f:\n",
    "                json.dump(out_jsons[keys], f)\n",
    "            print(f'{out_json_path} successfully saved')\n",
    "        \n",
    "        if return_dict:\n",
    "            return out_jsons\n",
    "\n",
    "    def read_json(self, json_fn: str):\n",
    "        json_path = os.path.join(self.base_path, json_fn)\n",
    "        with open(json_path, 'r') as f:\n",
    "            schema = json.load(f)\n",
    "        print(f'{json_path} successfully loaded')\n",
    "        return schema\n",
    "\n",
    "    def write_json(self, json_fn: str, schema: dict):\n",
    "        json_path = os.path.join(self.base_path, json_fn)\n",
    "        with open(json_path, 'w') as f:\n",
    "            json.dump(schema, f, indent=4)\n",
    "        print(f'{json_path} successfully saved')\n",
    "\n",
    "    def resolve_references(self, schema):\n",
    "        def resolve_node(node):\n",
    "            if isinstance(node, dict):\n",
    "                if '$ref' in node:\n",
    "                    ref_path = node['$ref']\n",
    "                    ref_file, ref_key = ref_path.split('#')\n",
    "                    ref_file = ref_file.strip()\n",
    "                    ref_key = ref_key.strip('/')\n",
    "\n",
    "                    if ref_file:\n",
    "                        ref_file_path = os.path.join(self.base_path, ref_file)\n",
    "                        with open(ref_file_path, 'r') as f:\n",
    "                            ref_content = json.load(f)\n",
    "                    else:\n",
    "                        ref_content = schema\n",
    "\n",
    "                    for part in ref_key.split('/'):\n",
    "                        ref_content = ref_content[part]\n",
    "\n",
    "                    resolved_content = resolve_node(ref_content)\n",
    "                    # Merge resolved content with the current node, excluding the $ref key\n",
    "                    return {**resolved_content, **{k: resolve_node(v) for k, v in node.items() if k != '$ref'}}\n",
    "                else:\n",
    "                    return {k: resolve_node(v) for k, v in node.items()}\n",
    "            elif isinstance(node, list):\n",
    "                return [resolve_node(item) for item in node]\n",
    "            else:\n",
    "                return node\n",
    "\n",
    "        return resolve_node(schema)\n",
    "\n",
    "    def redefine_ref_path(self, match_str: str, replace_str: str, schema: dict):\n",
    "        \"\"\"\n",
    "        Recursively replaces $ref paths in a JSON schema.\n",
    "\n",
    "        Args:\n",
    "        - match_str (str): The string to match in the $ref path.\n",
    "        - replace_str (str): The string to replace the match_str with.\n",
    "        - schema (dict): The JSON schema to modify.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The modified JSON schema with updated $ref paths.\n",
    "        \"\"\"\n",
    "        def replace_refs(node):\n",
    "            if isinstance(node, dict):\n",
    "                if '$ref' in node and match_str in node['$ref']:\n",
    "                    node['$ref'] = node['$ref'].replace(match_str, replace_str)\n",
    "                return {k: replace_refs(v) for k, v in node.items()}\n",
    "            elif isinstance(node, list):\n",
    "                return [replace_refs(item) for item in node]\n",
    "            else:\n",
    "                return node\n",
    "\n",
    "        return replace_refs(schema)\n",
    "\n",
    "    def resolve_refs(self, schema_fn, ref_fn):\n",
    "        \"\"\"\n",
    "        Resolves references in a JSON schema file using definitions from another JSON file.\n",
    "\n",
    "        Args:\n",
    "        - schema_fn (str): The name of the schema JSON file to resolve.\n",
    "        - ref_fn (str): The name of the JSON file with the reference definitions.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The resolved JSON schema.\n",
    "        \"\"\"\n",
    "        # Read JSON files\n",
    "        schema_obj = self.read_json(schema_fn)\n",
    "        ref_obj = self.read_json(ref_fn)\n",
    "\n",
    "        # Redefine $ref paths in schema_obj if necessary\n",
    "        if '_definitions.json' in schema_fn:\n",
    "            schema_obj = self.redefine_ref_path('_terms.yaml', '_terms.json', schema_obj)\n",
    "        else:\n",
    "            schema_obj = self.redefine_ref_path('_definitions.yaml', '_definitions_[resolved].json', schema_obj)\n",
    "            schema_obj = self.redefine_ref_path('.yaml', '.json', schema_obj)\n",
    "\n",
    "        # Resolve references\n",
    "        resolved_schema = self.resolve_references(schema_obj)\n",
    "        \n",
    "        # Write resolved schema\n",
    "        resolved_schema_file = schema_fn.replace('.json', '_[resolved].json')\n",
    "        self.write_json(resolved_schema_file, resolved_schema)\n",
    "\n",
    "        if resolved_schema is not None:\n",
    "            return print(f'=== {schema_fn} successfully resolved ===')\n",
    "        else:\n",
    "            return print(f'=== {schema_fn} failed to resolve ===')\n",
    "\n",
    "    def move_resolved_schemas(self, target_dir: str):\n",
    "        \"\"\"\n",
    "        Moves resolved schemas to a new directory.\n",
    "\n",
    "        Args:\n",
    "        - target_dir (str): The target directory where the resolved schemas should be moved.\n",
    "        \"\"\"\n",
    "        os.makedirs(target_dir, exist_ok=True)\n",
    "        ref_files = [f for f in os.listdir(self.base_path) if '[resolved].json' in f]\n",
    "        for f in ref_files:\n",
    "            shutil.move(os.path.join(self.base_path, f), os.path.join(target_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing demographic.yaml to json\n",
      "writing project.yaml to json\n",
      "writing serum_marker_assay.yaml to json\n",
      "writing alignment_workflow.yaml to json\n",
      "writing lipidomics_assay.yaml to json\n",
      "writing metabolomics_file.yaml to json\n",
      "writing acknowledgement.yaml to json\n",
      "writing medical_history.yaml to json\n",
      "writing _definitions.yaml to json\n",
      "writing _settings.yaml to json\n",
      "writing blood_pressure_test.yaml to json\n",
      "writing genomics_assay.yaml to json\n",
      "writing variant_file.yaml to json\n",
      "writing program.yaml to json\n",
      "writing lipidomics_mapping.yaml to json\n",
      "writing serum_marker_file.yaml to json\n",
      "writing proteomics_assay.yaml to json\n",
      "writing sample.yaml to json\n",
      "writing unaligned_reads_file.yaml to json\n",
      "writing _terms.yaml to json\n",
      "writing aligned_reads_index_file.yaml to json\n",
      "writing variant_workflow.yaml to json\n",
      "writing proteomics_file.yaml to json\n",
      "writing exposure.yaml to json\n",
      "writing metabolomics_assay.yaml to json\n",
      "writing lipidomics_file.yaml to json\n",
      "writing aligned_reads_file.yaml to json\n",
      "writing lab_result.yaml to json\n",
      "writing medication.yaml to json\n",
      "writing publication.yaml to json\n",
      "writing subject.yaml to json\n",
      "writing core_metadata_collection.yaml to json\n",
      "../output/schema/json/unresolved/demographic.json successfully saved\n",
      "../output/schema/json/unresolved/project.json successfully saved\n",
      "../output/schema/json/unresolved/serum_marker_assay.json successfully saved\n",
      "../output/schema/json/unresolved/alignment_workflow.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/metabolomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/acknowledgement.json successfully saved\n",
      "../output/schema/json/unresolved/medical_history.json successfully saved\n",
      "../output/schema/json/unresolved/_definitions.json successfully saved\n",
      "../output/schema/json/unresolved/_settings.json successfully saved\n",
      "../output/schema/json/unresolved/blood_pressure_test.json successfully saved\n",
      "../output/schema/json/unresolved/genomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/variant_file.json successfully saved\n",
      "../output/schema/json/unresolved/program.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_mapping.json successfully saved\n",
      "../output/schema/json/unresolved/serum_marker_file.json successfully saved\n",
      "../output/schema/json/unresolved/proteomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/sample.json successfully saved\n",
      "../output/schema/json/unresolved/unaligned_reads_file.json successfully saved\n",
      "../output/schema/json/unresolved/_terms.json successfully saved\n",
      "../output/schema/json/unresolved/aligned_reads_index_file.json successfully saved\n",
      "../output/schema/json/unresolved/variant_workflow.json successfully saved\n",
      "../output/schema/json/unresolved/proteomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/exposure.json successfully saved\n",
      "../output/schema/json/unresolved/metabolomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/aligned_reads_file.json successfully saved\n",
      "../output/schema/json/unresolved/lab_result.json successfully saved\n",
      "../output/schema/json/unresolved/medication.json successfully saved\n",
      "../output/schema/json/unresolved/publication.json successfully saved\n",
      "../output/schema/json/unresolved/subject.json successfully saved\n",
      "../output/schema/json/unresolved/core_metadata_collection.json successfully saved\n",
      "../output/schema/json/unresolved/_definitions.json successfully loaded\n",
      "../output/schema/json/unresolved/_terms.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully saved\n",
      "=== _definitions.json successfully resolved ===\n",
      "_definitions_[resolved].json\n",
      "lipidomics_assay.json\n",
      "../output/schema/json/unresolved/lipidomics_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/lipidomics_assay_[resolved].json successfully saved\n",
      "=== lipidomics_assay.json successfully resolved ===\n",
      "acknowledgement.json\n",
      "../output/schema/json/unresolved/acknowledgement.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/acknowledgement_[resolved].json successfully saved\n",
      "=== acknowledgement.json successfully resolved ===\n",
      "metabolomics_file.json\n",
      "../output/schema/json/unresolved/metabolomics_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/metabolomics_file_[resolved].json successfully saved\n",
      "=== metabolomics_file.json successfully resolved ===\n",
      "demographic.json\n",
      "../output/schema/json/unresolved/demographic.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/demographic_[resolved].json successfully saved\n",
      "=== demographic.json successfully resolved ===\n",
      "alignment_workflow.json\n",
      "../output/schema/json/unresolved/alignment_workflow.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/alignment_workflow_[resolved].json successfully saved\n",
      "=== alignment_workflow.json successfully resolved ===\n",
      "serum_marker_assay.json\n",
      "../output/schema/json/unresolved/serum_marker_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/serum_marker_assay_[resolved].json successfully saved\n",
      "=== serum_marker_assay.json successfully resolved ===\n",
      "project.json\n",
      "../output/schema/json/unresolved/project.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/project_[resolved].json successfully saved\n",
      "=== project.json successfully resolved ===\n",
      "program.json\n",
      "../output/schema/json/unresolved/program.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/program_[resolved].json successfully saved\n",
      "=== program.json successfully resolved ===\n",
      "variant_file.json\n",
      "../output/schema/json/unresolved/variant_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/variant_file_[resolved].json successfully saved\n",
      "=== variant_file.json successfully resolved ===\n",
      "genomics_assay.json\n",
      "../output/schema/json/unresolved/genomics_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/genomics_assay_[resolved].json successfully saved\n",
      "=== genomics_assay.json successfully resolved ===\n",
      "blood_pressure_test.json\n",
      "../output/schema/json/unresolved/blood_pressure_test.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/blood_pressure_test_[resolved].json successfully saved\n",
      "=== blood_pressure_test.json successfully resolved ===\n",
      "serum_marker_file.json\n",
      "../output/schema/json/unresolved/serum_marker_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/serum_marker_file_[resolved].json successfully saved\n",
      "=== serum_marker_file.json successfully resolved ===\n",
      "proteomics_assay.json\n",
      "../output/schema/json/unresolved/proteomics_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/proteomics_assay_[resolved].json successfully saved\n",
      "=== proteomics_assay.json successfully resolved ===\n",
      "lipidomics_mapping.json\n",
      "../output/schema/json/unresolved/lipidomics_mapping.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/lipidomics_mapping_[resolved].json successfully saved\n",
      "=== lipidomics_mapping.json successfully resolved ===\n",
      "medical_history.json\n",
      "../output/schema/json/unresolved/medical_history.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/medical_history_[resolved].json successfully saved\n",
      "=== medical_history.json successfully resolved ===\n",
      "proteomics_file.json\n",
      "../output/schema/json/unresolved/proteomics_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/proteomics_file_[resolved].json successfully saved\n",
      "=== proteomics_file.json successfully resolved ===\n",
      "variant_workflow.json\n",
      "../output/schema/json/unresolved/variant_workflow.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/variant_workflow_[resolved].json successfully saved\n",
      "=== variant_workflow.json successfully resolved ===\n",
      "aligned_reads_index_file.json\n",
      "../output/schema/json/unresolved/aligned_reads_index_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/aligned_reads_index_file_[resolved].json successfully saved\n",
      "=== aligned_reads_index_file.json successfully resolved ===\n",
      "metabolomics_assay.json\n",
      "../output/schema/json/unresolved/metabolomics_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/metabolomics_assay_[resolved].json successfully saved\n",
      "=== metabolomics_assay.json successfully resolved ===\n",
      "exposure.json\n",
      "../output/schema/json/unresolved/exposure.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/exposure_[resolved].json successfully saved\n",
      "=== exposure.json successfully resolved ===\n",
      "unaligned_reads_file.json\n",
      "../output/schema/json/unresolved/unaligned_reads_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/unaligned_reads_file_[resolved].json successfully saved\n",
      "=== unaligned_reads_file.json successfully resolved ===\n",
      "sample.json\n",
      "../output/schema/json/unresolved/sample.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/sample_[resolved].json successfully saved\n",
      "=== sample.json successfully resolved ===\n",
      "subject.json\n",
      "../output/schema/json/unresolved/subject.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/subject_[resolved].json successfully saved\n",
      "=== subject.json successfully resolved ===\n",
      "publication.json\n",
      "../output/schema/json/unresolved/publication.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/publication_[resolved].json successfully saved\n",
      "=== publication.json successfully resolved ===\n",
      "medication.json\n",
      "../output/schema/json/unresolved/medication.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/medication_[resolved].json successfully saved\n",
      "=== medication.json successfully resolved ===\n",
      "core_metadata_collection.json\n",
      "../output/schema/json/unresolved/core_metadata_collection.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/core_metadata_collection_[resolved].json successfully saved\n",
      "=== core_metadata_collection.json successfully resolved ===\n",
      "lipidomics_file.json\n",
      "../output/schema/json/unresolved/lipidomics_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/lipidomics_file_[resolved].json successfully saved\n",
      "=== lipidomics_file.json successfully resolved ===\n",
      "lab_result.json\n",
      "../output/schema/json/unresolved/lab_result.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/lab_result_[resolved].json successfully saved\n",
      "=== lab_result.json successfully resolved ===\n",
      "aligned_reads_file.json\n",
      "../output/schema/json/unresolved/aligned_reads_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/aligned_reads_file_[resolved].json successfully saved\n",
      "=== aligned_reads_file.json successfully resolved ===\n"
     ]
    }
   ],
   "source": [
    "# if this path exists, execute rmtree:\n",
    "output_dir = '../output/schema/json/unresolved'\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the SchemaValidator\n",
    "validator = SchemaValidator(base_path=output_dir, bundle_json_path='../output/schema/json/schema_dev.json')\n",
    "\n",
    "# Splitting jsons\n",
    "validator.split_bundle_json(output_dir)\n",
    "\n",
    "# resolving refs\n",
    "validator.resolve_refs('_definitions.json', '_terms.json')\n",
    "\n",
    "# resolving other schemas\n",
    "jsonfn = [fn for fn in os.listdir(output_dir) if not fn.startswith('_')] # excluding any filename starting with _\n",
    "refFn = '_definitions_[resolved].json'\n",
    "print(refDir)\n",
    "for fn in jsonfn:\n",
    "    print(fn)\n",
    "    validator.resolve_refs(fn, refFn)\n",
    "\n",
    "# moving resolved schemas\n",
    "target_dir = os.path.join(output_dir, '../resolved')\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "validator.move_resolved_schemas(target_dir=target_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From this point forward I have loaded the class:\n",
    "- 2024-06-07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing demographic.yaml to json\n",
      "writing project.yaml to json\n",
      "writing serum_marker_assay.yaml to json\n",
      "writing alignment_workflow.yaml to json\n",
      "writing lipidomics_assay.yaml to json\n",
      "writing metabolomics_file.yaml to json\n",
      "writing acknowledgement.yaml to json\n",
      "writing medical_history.yaml to json\n",
      "writing _definitions.yaml to json\n",
      "writing _settings.yaml to json\n",
      "writing blood_pressure_test.yaml to json\n",
      "writing genomics_assay.yaml to json\n",
      "writing variant_file.yaml to json\n",
      "writing program.yaml to json\n",
      "writing lipidomics_mapping.yaml to json\n",
      "writing serum_marker_file.yaml to json\n",
      "writing proteomics_assay.yaml to json\n",
      "writing sample.yaml to json\n",
      "writing unaligned_reads_file.yaml to json\n",
      "writing _terms.yaml to json\n",
      "writing aligned_reads_index_file.yaml to json\n",
      "writing variant_workflow.yaml to json\n",
      "writing proteomics_file.yaml to json\n",
      "writing exposure.yaml to json\n",
      "writing metabolomics_assay.yaml to json\n",
      "writing lipidomics_file.yaml to json\n",
      "writing aligned_reads_file.yaml to json\n",
      "writing lab_result.yaml to json\n",
      "writing medication.yaml to json\n",
      "writing publication.yaml to json\n",
      "writing subject.yaml to json\n",
      "writing core_metadata_collection.yaml to json\n",
      "../output/schema/json/unresolved/demographic.json successfully saved\n",
      "../output/schema/json/unresolved/project.json successfully saved\n",
      "../output/schema/json/unresolved/serum_marker_assay.json successfully saved\n",
      "../output/schema/json/unresolved/alignment_workflow.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/metabolomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/acknowledgement.json successfully saved\n",
      "../output/schema/json/unresolved/medical_history.json successfully saved\n",
      "../output/schema/json/unresolved/_definitions.json successfully saved\n",
      "../output/schema/json/unresolved/_settings.json successfully saved\n",
      "../output/schema/json/unresolved/blood_pressure_test.json successfully saved\n",
      "../output/schema/json/unresolved/genomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/variant_file.json successfully saved\n",
      "../output/schema/json/unresolved/program.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_mapping.json successfully saved\n",
      "../output/schema/json/unresolved/serum_marker_file.json successfully saved\n",
      "../output/schema/json/unresolved/proteomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/sample.json successfully saved\n",
      "../output/schema/json/unresolved/unaligned_reads_file.json successfully saved\n",
      "../output/schema/json/unresolved/_terms.json successfully saved\n",
      "../output/schema/json/unresolved/aligned_reads_index_file.json successfully saved\n",
      "../output/schema/json/unresolved/variant_workflow.json successfully saved\n",
      "../output/schema/json/unresolved/proteomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/exposure.json successfully saved\n",
      "../output/schema/json/unresolved/metabolomics_assay.json successfully saved\n",
      "../output/schema/json/unresolved/lipidomics_file.json successfully saved\n",
      "../output/schema/json/unresolved/aligned_reads_file.json successfully saved\n",
      "../output/schema/json/unresolved/lab_result.json successfully saved\n",
      "../output/schema/json/unresolved/medication.json successfully saved\n",
      "../output/schema/json/unresolved/publication.json successfully saved\n",
      "../output/schema/json/unresolved/subject.json successfully saved\n",
      "../output/schema/json/unresolved/core_metadata_collection.json successfully saved\n",
      "../output/schema/json/unresolved/_definitions.json successfully loaded\n",
      "../output/schema/json/unresolved/_terms.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully saved\n",
      "=== _definitions.json successfully resolved ===\n",
      "_definitions_[resolved].json\n",
      "lipidomics_assay.json\n",
      "../output/schema/json/unresolved/lipidomics_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/lipidomics_assay_[resolved].json successfully saved\n",
      "=== lipidomics_assay.json successfully resolved ===\n",
      "acknowledgement.json\n",
      "../output/schema/json/unresolved/acknowledgement.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/acknowledgement_[resolved].json successfully saved\n",
      "=== acknowledgement.json successfully resolved ===\n",
      "metabolomics_file.json\n",
      "../output/schema/json/unresolved/metabolomics_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/metabolomics_file_[resolved].json successfully saved\n",
      "=== metabolomics_file.json successfully resolved ===\n",
      "demographic.json\n",
      "../output/schema/json/unresolved/demographic.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/demographic_[resolved].json successfully saved\n",
      "=== demographic.json successfully resolved ===\n",
      "alignment_workflow.json\n",
      "../output/schema/json/unresolved/alignment_workflow.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/alignment_workflow_[resolved].json successfully saved\n",
      "=== alignment_workflow.json successfully resolved ===\n",
      "serum_marker_assay.json\n",
      "../output/schema/json/unresolved/serum_marker_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/serum_marker_assay_[resolved].json successfully saved\n",
      "=== serum_marker_assay.json successfully resolved ===\n",
      "project.json\n",
      "../output/schema/json/unresolved/project.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/project_[resolved].json successfully saved\n",
      "=== project.json successfully resolved ===\n",
      "program.json\n",
      "../output/schema/json/unresolved/program.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/program_[resolved].json successfully saved\n",
      "=== program.json successfully resolved ===\n",
      "variant_file.json\n",
      "../output/schema/json/unresolved/variant_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/variant_file_[resolved].json successfully saved\n",
      "=== variant_file.json successfully resolved ===\n",
      "genomics_assay.json\n",
      "../output/schema/json/unresolved/genomics_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/genomics_assay_[resolved].json successfully saved\n",
      "=== genomics_assay.json successfully resolved ===\n",
      "blood_pressure_test.json\n",
      "../output/schema/json/unresolved/blood_pressure_test.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/blood_pressure_test_[resolved].json successfully saved\n",
      "=== blood_pressure_test.json successfully resolved ===\n",
      "serum_marker_file.json\n",
      "../output/schema/json/unresolved/serum_marker_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/serum_marker_file_[resolved].json successfully saved\n",
      "=== serum_marker_file.json successfully resolved ===\n",
      "proteomics_assay.json\n",
      "../output/schema/json/unresolved/proteomics_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/proteomics_assay_[resolved].json successfully saved\n",
      "=== proteomics_assay.json successfully resolved ===\n",
      "lipidomics_mapping.json\n",
      "../output/schema/json/unresolved/lipidomics_mapping.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/lipidomics_mapping_[resolved].json successfully saved\n",
      "=== lipidomics_mapping.json successfully resolved ===\n",
      "medical_history.json\n",
      "../output/schema/json/unresolved/medical_history.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/medical_history_[resolved].json successfully saved\n",
      "=== medical_history.json successfully resolved ===\n",
      "proteomics_file.json\n",
      "../output/schema/json/unresolved/proteomics_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/proteomics_file_[resolved].json successfully saved\n",
      "=== proteomics_file.json successfully resolved ===\n",
      "variant_workflow.json\n",
      "../output/schema/json/unresolved/variant_workflow.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/variant_workflow_[resolved].json successfully saved\n",
      "=== variant_workflow.json successfully resolved ===\n",
      "aligned_reads_index_file.json\n",
      "../output/schema/json/unresolved/aligned_reads_index_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/aligned_reads_index_file_[resolved].json successfully saved\n",
      "=== aligned_reads_index_file.json successfully resolved ===\n",
      "metabolomics_assay.json\n",
      "../output/schema/json/unresolved/metabolomics_assay.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/metabolomics_assay_[resolved].json successfully saved\n",
      "=== metabolomics_assay.json successfully resolved ===\n",
      "exposure.json\n",
      "../output/schema/json/unresolved/exposure.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/exposure_[resolved].json successfully saved\n",
      "=== exposure.json successfully resolved ===\n",
      "unaligned_reads_file.json\n",
      "../output/schema/json/unresolved/unaligned_reads_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/unaligned_reads_file_[resolved].json successfully saved\n",
      "=== unaligned_reads_file.json successfully resolved ===\n",
      "sample.json\n",
      "../output/schema/json/unresolved/sample.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/sample_[resolved].json successfully saved\n",
      "=== sample.json successfully resolved ===\n",
      "subject.json\n",
      "../output/schema/json/unresolved/subject.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/subject_[resolved].json successfully saved\n",
      "=== subject.json successfully resolved ===\n",
      "publication.json\n",
      "../output/schema/json/unresolved/publication.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/publication_[resolved].json successfully saved\n",
      "=== publication.json successfully resolved ===\n",
      "medication.json\n",
      "../output/schema/json/unresolved/medication.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/medication_[resolved].json successfully saved\n",
      "=== medication.json successfully resolved ===\n",
      "core_metadata_collection.json\n",
      "../output/schema/json/unresolved/core_metadata_collection.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/core_metadata_collection_[resolved].json successfully saved\n",
      "=== core_metadata_collection.json successfully resolved ===\n",
      "lipidomics_file.json\n",
      "../output/schema/json/unresolved/lipidomics_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/lipidomics_file_[resolved].json successfully saved\n",
      "=== lipidomics_file.json successfully resolved ===\n",
      "lab_result.json\n",
      "../output/schema/json/unresolved/lab_result.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/lab_result_[resolved].json successfully saved\n",
      "=== lab_result.json successfully resolved ===\n",
      "aligned_reads_file.json\n",
      "../output/schema/json/unresolved/aligned_reads_file.json successfully loaded\n",
      "../output/schema/json/unresolved/_definitions_[resolved].json successfully loaded\n",
      "../output/schema/json/unresolved/aligned_reads_file_[resolved].json successfully saved\n",
      "=== aligned_reads_file.json successfully resolved ===\n"
     ]
    }
   ],
   "source": [
    "# Refresh the gen3schemadev library\n",
    "import importlib\n",
    "import gen3schemadev\n",
    "importlib.reload(gen3schemadev)\n",
    "\n",
    "# if this path exists, execute rmtree:\n",
    "output_dir = '../output/schema/json/unresolved'\n",
    "if os.path.exists(output_dir):\n",
    "    shutil.rmtree(output_dir, ignore_errors=True)\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the SchemaValidator\n",
    "validator = SchemaValidator(base_path=output_dir, bundle_json_path='../output/schema/json/schema_dev.json')\n",
    "\n",
    "# Splitting jsons\n",
    "validator.split_bundle_json(output_dir)\n",
    "\n",
    "# resolving refs\n",
    "validator.resolve_refs('_definitions.json', '_terms.json')\n",
    "\n",
    "# resolving other schemas\n",
    "jsonfn = [fn for fn in os.listdir(output_dir) if not fn.startswith('_')] # excluding any filename starting with _\n",
    "refFn = '_definitions_[resolved].json'\n",
    "print(refDir)\n",
    "for fn in jsonfn:\n",
    "    print(fn)\n",
    "    validator.resolve_refs(fn, refFn)\n",
    "\n",
    "# moving resolved schemas\n",
    "target_dir = os.path.join(output_dir, '../resolved')\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "validator.move_resolved_schemas(target_dir=target_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now time to add in the data parser and validator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
